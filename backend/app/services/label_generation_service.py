"""
Label Generation Service - Scorecard-Based Ground Truth Labels

This service generates ground truth labels using the expert scorecard.
Labels are generated by thresholding scorecard scores to identify the riskiest parties.
"""

from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime
import numpy as np
from sqlalchemy.orm import Session

from app.scorecard.scorecard_engine import ScorecardEngine
from app.models.models import GroundTruthLabel, Party


class LabelGenerationService:
    """
    Service for generating ground truth labels from scorecard scores.
    
    This creates the initial training data by:
    1. Computing scorecard scores for all parties
    2. Finding threshold for target default rate
    3. Labeling bottom X% as defaults
    """
    
    def __init__(self, db: Session, scorecard_version: str = '1.0'):
        """Initialize with database session and scorecard version."""
        self.db = db
        self.scorecard_version = scorecard_version
        self.engine = ScorecardEngine(version=scorecard_version)
    
    def generate_labels_from_scorecard(
        self,
        features_list: List[Dict[str, Any]],
        party_ids: List[int],
        target_default_rate: float = 0.05,
        batch_id: str = None,
    ) -> Dict[str, Any]:
        """
        Generate ground truth labels by thresholding scorecard scores.
        
        Args:
            features_list: List of feature dictionaries (one per party)
            party_ids: Corresponding party IDs
            target_default_rate: Fraction of parties to label as defaults (bottom X%)
            batch_id: Dataset batch identifier
            
        Returns:
            Summary of label generation including counts and threshold used
        """
        if len(features_list) != len(party_ids):
            raise ValueError("features_list and party_ids must have same length")
        
        # Compute scorecard scores for all parties
        scores = []
        score_results = []
        for features in features_list:
            result = self.engine.compute_scorecard_score(features)
            scores.append(result['score'])
            score_results.append(result)
        
        # Determine threshold for target default rate
        threshold = self.determine_default_threshold(scores, target_default_rate)
        
        # Generate binary labels
        labels_created = 0
        defaults_count = 0
        
        for i, (party_id, score, result) in enumerate(zip(party_ids, scores, score_results)):
            will_default = 1 if score < threshold else 0
            risk_level = self._score_to_risk_level(score)
            
            # Check for existing label
            existing = self.db.query(GroundTruthLabel).filter(
                GroundTruthLabel.party_id == party_id
            ).first()
            
            if existing:
                # Update existing
                existing.will_default = will_default
                existing.risk_level = risk_level
                existing.label_source = 'scorecard'
                existing.scorecard_version = self.scorecard_version
                existing.scorecard_raw_score = score
                existing.label_confidence = 0.5  # Synthetic labels get 50% confidence
                existing.dataset_batch = batch_id
                existing.reason = f"Scorecard v{self.scorecard_version} score: {score:.1f}"
            else:
                # Create new
                label = GroundTruthLabel(
                    party_id=party_id,
                    will_default=will_default,
                    risk_level=risk_level,
                    label_source='scorecard',
                    scorecard_version=self.scorecard_version,
                    scorecard_raw_score=score,
                    label_confidence=0.5,
                    dataset_batch=batch_id,
                    reason=f"Scorecard v{self.scorecard_version} score: {score:.1f}",
                )
                self.db.add(label)
            
            labels_created += 1
            if will_default:
                defaults_count += 1
        
        self.db.commit()
        
        actual_default_rate = defaults_count / labels_created if labels_created > 0 else 0
        
        return {
            'labels_created': labels_created,
            'defaults_count': defaults_count,
            'non_defaults_count': labels_created - defaults_count,
            'actual_default_rate': actual_default_rate,
            'target_default_rate': target_default_rate,
            'threshold_score': threshold,
            'scorecard_version': self.scorecard_version,
            'batch_id': batch_id,
            'score_distribution': {
                'min': min(scores),
                'max': max(scores),
                'mean': np.mean(scores),
                'median': np.median(scores),
            }
        }
    
    def determine_default_threshold(
        self, 
        scores: List[float], 
        target_default_rate: float = 0.05
    ) -> float:
        """
        Calculate the score threshold that produces the target default rate.
        
        Args:
            scores: List of scorecard scores
            target_default_rate: Target fraction of defaults (e.g., 0.05 for 5%)
            
        Returns:
            Score threshold below which parties are labeled as defaults
        """
        if not scores:
            return 0
        
        # Find the percentile that gives target default rate
        # Bottom 5% means 5th percentile
        percentile = target_default_rate * 100
        threshold = np.percentile(scores, percentile)
        
        return threshold
    
    def _score_to_risk_level(self, score: float) -> str:
        """Convert numeric score to risk level category."""
        if score >= 700:
            return 'low'
        elif score >= 550:
            return 'medium'
        else:
            return 'high'
    
    def generate_labels_for_batch(
        self,
        batch_id: str,
        target_default_rate: float = 0.05,
    ) -> Dict[str, Any]:
        """
        Generate labels for all parties in a specific batch.
        
        Args:
            batch_id: The batch identifier
            target_default_rate: Target default rate
            
        Returns:
            Summary of label generation
        """
        from app.services.feature_pipeline_service import FeaturePipelineService
        
        # Get all parties in batch
        parties = self.db.query(Party).filter(Party.batch_id == batch_id).all()
        
        if not parties:
            raise ValueError(f"No parties found for batch {batch_id}")
        
        # Extract features for each party
        feature_svc = FeaturePipelineService(self.db)
        features_list = []
        party_ids = []
        
        for party in parties:
            features = feature_svc.extract_features(party.id)
            features_list.append(features)
            party_ids.append(party.id)
        
        return self.generate_labels_from_scorecard(
            features_list=features_list,
            party_ids=party_ids,
            target_default_rate=target_default_rate,
            batch_id=batch_id,
        )


class LabelMixingPolicy:
    """
    Policy for blending scorecard-generated and observed labels.
    
    As real default data accumulates, gradually transition from
    purely scorecard labels to observed labels.
    """
    
    @staticmethod
    def get_mix_ratio(months_of_data: int) -> Dict[str, float]:
        """
        Determine label mixing ratio based on data maturity.
        
        Returns:
            Dictionary with 'scorecard' and 'observed' weights (sum to 1.0)
        """
        if months_of_data < 3:
            return {'scorecard': 1.0, 'observed': 0.0}
        elif months_of_data < 6:
            return {'scorecard': 0.7, 'observed': 0.3}
        elif months_of_data < 12:
            return {'scorecard': 0.3, 'observed': 0.7}
        else:
            return {'scorecard': 0.1, 'observed': 0.9}
    
    @staticmethod
    def blend_labels(
        scorecard_labels: List[int],
        observed_labels: List[int],
        mix_ratio: Dict[str, float],
    ) -> List[int]:
        """
        Blend scorecard and observed labels according to mixing policy.
        
        For each sample, probabilistically choose scorecard or observed label.
        """
        blended = []
        sc_weight = mix_ratio['scorecard']
        
        for sc_label, obs_label in zip(scorecard_labels, observed_labels):
            if obs_label is not None:
                # If observed exists, use mix probability
                use_scorecard = np.random.random() < sc_weight
                blended.append(sc_label if use_scorecard else obs_label)
            else:
                # No observed label, use scorecard
                blended.append(sc_label)
        
        return blended
